alpha <- armijo(w, grad, fn, miu, beta, X, y)
w <- w + alpha * grad
# Terminate if max_iter reached
if (step_count >= max_iter) {
break
}
step_count <- step_count + 1
##### Print intermediate progress
if (step_count %% steps == 0) {
cat("Step: ", step_count, "\t", "Gradient Norm: ", vnorm(grad), "\t",
"Function Value: ", fn(w, X, y), "\n")
}
}
cat("Armijo Gradient Descent algorithm terminated after step: ", step_count, "\t",
"The norm of the gradient at the solution vector is: ",
vnorm(grad),"\t", "The function Value at this point is: ",
fn(w, X, y), "\n")
return(w)
}
stochastic_gradient_descent <- function(fn, gradient_fn, X, y,
initial_soln = NA, tolerance = 0.01,
max_iter = 5e6, batch_size = 10,
steps = 1e4) {
set.seed(1)
if (any(is.na(initial_soln))) {
# If no initial stating point is provided, use the column mean from X
initial_soln <- apply(X, 2, mean)
}
w <- initial_soln
step_count <- 1
repeat {
# Perform Batch Stochastic Gradient Descent
rand <- sample.int(nrow(X), batch_size)
X_rand <- X[rand,]
y_rand <- y[rand]
grad <- -1 * gradient_fn(w, X_rand, y_rand)
##### Step Strategy code: 3 strategies
# 1. Armijo over sampled data
# alpha <- armijo(w, grad, fn, miu, beta, X_rand, y_rand)
# 2. Decreasing step size
alpha <- 1 / (step_count)
# 3. Fixed step size
# alpha <- 0.01
# Update current solution vector according to best step size
w <- w + alpha * grad
##### Termination Code
# Terminate if norm(z) = norm(alpha * grad) < tolerance
# Explanation: (w_prev + alpha * grad) - w_prev = alpha * grad
if (vnorm(alpha * grad) < tolerance) {
break
}
# Terminate if max_iter reached
if (step_count >= max_iter) {
break
} else if (step_count %% steps == 0) {
# Check the full gradient once for termination as well
full_grad <- gradient_fn(w, X, y)
if (vnorm(full_grad) < tolerance) {
break
}
}
step_count <- step_count + 1
##### Print intermediate progress
if (step_count %% steps == 0) {
cat("Step: ", step_count, "\t", "Gradient Norm: ", vnorm(full_grad), "\t",
"Function Value: ", fn(w, X, y), "\n")
}
}
cat("Stochastic Descent algorithm terminated after step: ", step_count, "\t",
"The norm of the gradient at the solution vector is: ",
vnorm(gradient_fn(w, X, y)),"\t", "The function Value at this point is: ",
fn(w, X, y), "\n")
return(w)
}
initial_solution <- gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e10, tolerance = 1,
steps = 100)
w1 <- gradient_descent(negative_log_likelihood, log_likelihood_gradient, X, y,
max_iter = 1e10, tolerance = 0.1, steps = 100,
initial_soln = initial_solution)
w2 <- gradient_descent(negative_log_likelihood, log_likelihood_gradient, X, y,
max_iter = 1e10, tolerance = 0.01, steps = 100,
initial_soln = initial_solution)
w3 <- gradient_descent(negative_log_likelihood, log_likelihood_gradient, X, y,
max_iter = 1e10, tolerance = 0.001, steps = 100,
initial_soln = initial_solution)
# Tolerance of 0.1
# w <- "-0.1746065 -0.05051722 -0.1206086 1.374583 0.4741843 0.2723485 0.9762846 0.4874669 0.117596 0.1554076 -0.1089688 -0.1847528 -0.3397111 0.2463493 0.4773129 0.5202804 0.6017748 -0.1060247 0.1214716 0.3150037 0.2085816 0.1889787 0.5902577 0.5625421 -1.366462 0.06333712 -3.633486 0.2583389 -0.2186485 0.05374561 0.4642654 0.885991 -0.1508402 1.593478 -0.4230773 0.5080846 -0.3556661 0.4475978 -0.3070221 0.07440292 -0.2643855 -0.9859749 -0.7160725 -0.7780508 -0.4393918 -0.9371447 0.2685051 -0.8763407 -0.4087969 -0.1206357 0.2083893 0.7895759 1.488489 0.01841322 0.707425 0.05480608 0.3438694"
# Terminated after 11062 iterations
# Tolerance of 0.01
# Terminated on step:  23558
# Norm of the gradient vector is:  0.009991184
# w <- -0.1761564 -0.04962621 -0.1206154 1.389409 0.4741564 0.2727272 0.9761242 0.4872798 0.1173945 0.1553712 -0.1091901 -0.1843405 -0.340183 0.2461267 0.4756507 0.5199962 0.6020968 -0.1055466 0.1215346 0.3160012 0.2088256 0.1884311 0.5908408 0.5623483 -1.367746 0.06252843 -3.643156 0.2584692 -0.2194607 0.05336037 0.4652071 0.7743476 -0.1506623 1.712722 -0.4227526 0.5099904 -0.3571317 0.4476274 -0.3073559 0.07424671 -0.2653334 -0.9864043 -0.7169926 -0.7785895 -0.4397094 -0.9381548 0.2670068 -0.8795828 -0.4086444 -0.1206444 0.2082946 0.7898132 1.488323 0.01825253 0.7076676 0.05440677 0.344457
# Tolerance of 0.001
# Terminated on step:  35419
# Norm of the gradient vector is:  0.0009999632
# w <- -0.1762973 -0.04953911 -0.1206251 1.389897 0.4741599 0.2727307 0.9761239 0.487278 0.1173698 0.1553411 -0.1091933 -0.1843018 -0.3401904 0.2461194 0.4755507 0.5199708 0.6020795 -0.105477 0.121542 0.3161001 0.208848 0.1883944 0.5908917 0.5623477 -1.367852 0.06242671 -3.642979 0.2584719 -0.219463 0.0533753 0.4652805 0.7625147 -0.1506372 1.724002 -0.4226722 0.5102061 -0.3572873 0.4476902 -0.3073614 0.07418384 -0.26529 -0.9863997 -0.7169561 -0.7785882 -0.4397223 -0.9382309 0.2669903 -0.8797491 -0.4085925 -0.120622 0.2083277 0.7898445 1.488282 0.01825253 0.7077115 0.0543595 0.3444832
initial_solution <- gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e10, tolerance = 1,
steps = 100)
w1_sgd <- stochastic_gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e20, steps = 5e2,
tolerance = 0.1, batch_size = 10,
initial_soln = initial_solution)
w2_sgd <- stochastic_gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e20, steps = 5e2,
tolerance = 0.01, batch_size = 10,
initial_soln = initial_solution)
w3_sgd <- stochastic_gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e20, steps = 5e2,
tolerance = 0.001, batch_size = 10,
initial_soln = initial_solution)
initial_solution <- gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e10, tolerance = 1,
steps = 100)
w1 <- gradient_descent(negative_log_likelihood, log_likelihood_gradient, X, y,
max_iter = 1e10, tolerance = 0.1, steps = 100,
initial_soln = initial_solution)
cat(w)
cat(w1)
w2 <- gradient_descent(negative_log_likelihood, log_likelihood_gradient, X, y,
max_iter = 1e10, tolerance = 0.01, steps = 100,
initial_soln = w1)
vnorm(log_likelihood_gradient(w1, X, y))
w2 <- gradient_descent(negative_log_likelihood, log_likelihood_gradient, X, y,
max_iter = 1e10, tolerance = 0.01, steps = 100,
initial_soln = w1)
w3 <- gradient_descent(negative_log_likelihood, log_likelihood_gradient, X, y,
max_iter = 1e10, tolerance = 0.001, steps = 100,
initial_soln = w2)
w1_sgd <- stochastic_gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e20, steps = 5e2,
tolerance = 0.1, batch_size = 10,
initial_soln = initial_solution)
w2_sgd <- stochastic_gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e20, steps = 5e2,
tolerance = 0.01, batch_size = 10,
initial_soln = initial_solution)
w3_sgd <- stochastic_gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e20, steps = 5e2,
tolerance = 0.001, batch_size = 10,
initial_soln = initial_solution)
w1_sgd <- stochastic_gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e20, steps = 5e2,
tolerance = 0.00001, batch_size = 10,
initial_soln = initial_solution)
w1_sgd <- stochastic_gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e20, steps = 5e2,
tolerance = 0.1, batch_size = 5,
initial_soln = initial_solution)
w2_sgd <- stochastic_gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e20, steps = 5e2,
tolerance = 0.01, batch_size = 5,
initial_soln = initial_solution)
w3_sgd <- stochastic_gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e20, steps = 5e2,
tolerance = 0.001, batch_size = 5,
initial_soln = initial_solution)
w3_sgd <- stochastic_gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e20, steps = 5e2,
tolerance = 0.001, batch_size = 20,
initial_soln = initial_solution)
w2_sgd <- stochastic_gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e20, steps = 5e2,
tolerance = 0.01, batch_size = 20,
initial_soln = initial_solution)
w1_sgd <- stochastic_gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e20, steps = 5e2,
tolerance = 0.1, batch_size = 20,
initial_soln = initial_solution)
# set.seed(1)
if (any(is.na(initial_soln))) {
# If no initial stating point is provided, use the column mean from X
initial_soln <- apply(X, 2, mean)
}
stochastic_gradient_descent <- function(fn, gradient_fn, X, y,
initial_soln = NA, tolerance = 0.01,
max_iter = 5e6, batch_size = 10,
steps = 1e4) {
# set.seed(1)
if (any(is.na(initial_soln))) {
# If no initial stating point is provided, use the column mean from X
initial_soln <- apply(X, 2, mean)
}
w <- initial_soln
step_count <- 1
repeat {
# Perform Batch Stochastic Gradient Descent
rand <- sample.int(nrow(X), batch_size)
X_rand <- X[rand,]
y_rand <- y[rand]
grad <- -1 * gradient_fn(w, X_rand, y_rand)
##### Step Strategy code: 3 strategies
# 1. Armijo over sampled data
# alpha <- armijo(w, grad, fn, miu, beta, X_rand, y_rand)
# 2. Decreasing step size
alpha <- 1 / (step_count)
# 3. Fixed step size
# alpha <- 0.01
# Update current solution vector according to best step size
w <- w + alpha * grad
##### Termination Code
# Terminate if norm(z) = norm(alpha * grad) < tolerance
# Explanation: (w_prev + alpha * grad) - w_prev = alpha * grad
if (vnorm(alpha * grad) < tolerance) {
break
}
# Terminate if max_iter reached
if (step_count >= max_iter) {
break
} else if (step_count %% steps == 0) {
# Check the full gradient once for termination as well
full_grad <- gradient_fn(w, X, y)
if (vnorm(full_grad) < tolerance) {
break
}
}
step_count <- step_count + 1
##### Print intermediate progress
if (step_count %% steps == 0) {
cat("Step: ", step_count, "\t", "Gradient Norm: ", vnorm(full_grad), "\t",
"Function Value: ", fn(w, X, y), "\n")
}
}
cat("Stochastic Descent algorithm terminated after step: ", step_count, "\t",
"The norm of the gradient at the solution vector is: ",
vnorm(gradient_fn(w, X, y)),"\t", "The function Value at this point is: ",
fn(w, X, y), "\n")
return(w)
}
w1_sgd <- stochastic_gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e20, steps = 5e2,
tolerance = 0.1, batch_size = 20,
initial_soln = initial_solution)
w2_sgd <- stochastic_gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e20, steps = 5e2,
tolerance = 0.01, batch_size = 20,
initial_soln = initial_solution)
w3_sgd <- stochastic_gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e20, steps = 5e2,
tolerance = 0.001, batch_size = 20,
initial_soln = initial_solution)
stochastic_gradient_descent <- function(fn, gradient_fn, X, y,
initial_soln = NA, tolerance = 0.01,
max_iter = 5e6, batch_size = 10,
steps = 1e4) {
if (any(is.na(initial_soln))) {
# If no initial stating point is provided, use the column mean from X
initial_soln <- apply(X, 2, mean)
}
w <- initial_soln
step_count <- 1
repeat {
# Perform Batch Stochastic Gradient Descent
rand <- sample.int(nrow(X), batch_size)
X_rand <- X[rand,]
y_rand <- y[rand]
grad <- -1 * gradient_fn(w, X_rand, y_rand)
##### Step Strategy code: 3 strategies
# 1. Armijo over sampled data
# alpha <- armijo(w, grad, fn, miu, beta, X_rand, y_rand)
# 2. Decreasing step size
alpha <- 1 / (step_count)
# 3. Fixed step size
# alpha <- 0.01
# Update current solution vector according to best step size
w <- w + alpha * grad
##### Termination Code
# Terminate if norm(z) = norm(alpha * grad) < tolerance
# Explanation: (w_prev + alpha * grad) - w_prev = alpha * grad
if (vnorm(alpha * grad) < tolerance) {
break
}
# Terminate if max_iter reached
if (step_count >= max_iter) {
break
} else if (step_count %% steps == 0) {
# Check the full gradient once for termination as well
full_grad <- gradient_fn(w, X, y)
if (vnorm(full_grad) < tolerance) {
break
}
}
step_count <- step_count + 1
##### Print intermediate progress
if (step_count %% steps == 0) {
cat("Step: ", step_count, "\t", "Gradient Norm: ", vnorm(full_grad), "\t",
"Function Value: ", fn(w, X, y), "\n")
}
}
cat("Stochastic Descent algorithm terminated after step: ", step_count, "\t",
"The norm of the gradient at the solution vector is: ",
vnorm(gradient_fn(w, X, y)),"\t", "The function Value at this point is: ",
fn(w, X, y), "\n")
return(w)
}
w1_sgd <- stochastic_gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e20, steps = 5e2,
tolerance = 0.1, batch_size = 20,
initial_soln = initial_solution)
w2_sgd <- stochastic_gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e20, steps = 5e2,
tolerance = 0.01, batch_size = 20,
initial_soln = initial_solution)
w3_sgd <- stochastic_gradient_descent(negative_log_likelihood,
log_likelihood_gradient, X, y,
max_iter = 1e20, steps = 5e2,
tolerance = 0.001, batch_size = 20,
initial_soln = initial_solution)
dim(X)
sign(1)
sign(-1)
max(3, 0)
max(1:10, 5:15)
a <- 1:10
b <- 11:20
a
b
max(a, b)
?max
pmax(a, b)
abs(-1)
proximal_gradient <- function(fn, gradient_fn, X, y, initial_soln = NA,
tolerance = 0.01, max_iter = 500, steps = 50) {
if (any(is.na(initial_soln))) {
# If no initial stating point is provided, use the column mean from X
initial_soln <- apply(X, 2, mean)
}
w <- initial_soln
step_count <- 1
repeat {
grad <- -1 * gradient_fn(w, X, y)
w_prev <- w
w <- sign(w + (1/L)*grad) * pmax(abs(w + (1/L)*grad), 0)
if (vnorm(w - w_prev) < tolerance) {
break
}
# Terminate if max_iter reached
if (step_count >= max_iter) {
break
}
step_count <- step_count + 1
##### Print intermediate progress
if (step_count %% steps == 0) {
cat("Step: ", step_count, "\t", "Gradient Norm: ", vnorm(grad), "\t",
"Function Value: ", fn(w, X, y), "\n")
}
}
cat("Proximal Descent algorithm terminated after step: ", step_count, "\t",
"The norm of the gradient at the solution vector is: ",
vnorm(grad),"\t", "The function Value at this point is: ",
fn(w, X, y), "\n")
return(w)
}
library(rmatio)
email_full <- read.mat("HW1data.mat")
X <- email_full$Xtrain
y <- email_full$ytrain
X_test <- email_full$Xtest
y_test <- email_full$ytest
# Function to compute the Euclidean Norm of a given vector x
vnorm <- function(x) {
return(sqrt(c(x %*% x)))
}
# Objective Function: Negative Log Likelihood
negative_log_likelihood <- function(w, X, y) {
return(sum(log(1 + exp(-y * (X %*% w)))))
}
# Gradient Function
log_likelihood_gradient <- function(w, X, y) {
coefs <- -y / (1 + exp(y * as.vector(X %*% w)))
mat <- coefs * X
return(apply(mat, 2, sum))
}
proximal_gradient <- function(fn, gradient_fn, X, y, initial_soln = NA,
tolerance = 0.01, max_iter = 500, steps = 50) {
if (any(is.na(initial_soln))) {
# If no initial stating point is provided, use the column mean from X
initial_soln <- apply(X, 2, mean)
}
w <- initial_soln
step_count <- 1
repeat {
grad <- -1 * gradient_fn(w, X, y)
w_prev <- w
w <- sign(w + (1/L)*grad) * pmax(abs(w + (1/L)*grad), 0)
if (vnorm(w - w_prev) < tolerance) {
break
}
# Terminate if max_iter reached
if (step_count >= max_iter) {
break
}
step_count <- step_count + 1
##### Print intermediate progress
if (step_count %% steps == 0) {
cat("Step: ", step_count, "\t", "Gradient Norm: ", vnorm(grad), "\t",
"Function Value: ", fn(w, X, y), "\n")
}
}
cat("Proximal Descent algorithm terminated after step: ", step_count, "\t",
"The norm of the gradient at the solution vector is: ",
vnorm(grad),"\t", "The function Value at this point is: ",
fn(w, X, y), "\n")
return(w)
}
p1 <- proximal_gradient(negative_log_likelihood, log_likelihood_gradient, X, y,
max_iter = 1e10, tolerance = 0.1, steps = 100,
initial_soln = initial_solution)
a <- matrix(1:9, nrow=3)
a
a * a
sum(a)
proximal_gradient <- function(fn, gradient_fn, X, y, initial_soln = NA,
tolerance = 0.01, max_iter = 500, steps = 50) {
L <- (1/2) * sum(X * X)
if (any(is.na(initial_soln))) {
# If no initial stating point is provided, use the column mean from X
initial_soln <- apply(X, 2, mean)
}
w <- initial_soln
step_count <- 1
repeat {
grad <- -1 * gradient_fn(w, X, y)
w_prev <- w
w <- sign(w + (1/L)*grad) * pmax(abs(w + (1/L)*grad), 0)
if (vnorm(w - w_prev) < tolerance) {
break
}
# Terminate if max_iter reached
if (step_count >= max_iter) {
break
}
step_count <- step_count + 1
##### Print intermediate progress
if (step_count %% steps == 0) {
cat("Step: ", step_count, "\t", "Gradient Norm: ", vnorm(grad), "\t",
"Function Value: ", fn(w, X, y), "\n")
}
}
cat("Proximal Descent algorithm terminated after step: ", step_count, "\t",
"The norm of the gradient at the solution vector is: ",
vnorm(grad),"\t", "The function Value at this point is: ",
fn(w, X, y), "\n")
return(w)
}
p1 <- proximal_gradient(negative_log_likelihood, log_likelihood_gradient, X, y,
max_iter = 1e10, tolerance = 0.1, steps = 100,
initial_soln = initial_solution)
vnorm(log_likelihood_gradient(p1, X, y))
p1 <- proximal_gradient(negative_log_likelihood, log_likelihood_gradient, X, y,
max_iter = 1e10, tolerance = 0.1, steps = 100)
proximal_gradient <- function(fn, gradient_fn, X, y, initial_soln = NA,
tolerance = 0.01, max_iter = 500, steps = 50) {
L <- (1/2) * sum(X * X)
if (any(is.na(initial_soln))) {
# If no initial stating point is provided, use the column mean from X
initial_soln <- apply(X, 2, mean)
}
w <- initial_soln
step_count <- 1
repeat {
grad <- -1 * gradient_fn(w, X, y)
w_prev <- w
w <- sign(w + (1/L)*grad) * pmax(abs(w + (1/L)*grad), 0)
if (vnorm(w - w_prev) < tolerance) {
break
}
# Terminate if max_iter reached
if (step_count >= max_iter) {
break
}
step_count <- step_count + 1
##### Print intermediate progress
if (step_count %% steps == 0) {
cat("Step: ", step_count, "\t", "Gradient Norm: ", vnorm(grad), "\t",
"Function Value: ", fn(w, X, y), "\n")
}
}
cat("Proximal Descent algorithm terminated after step: ", step_count, "\t",
"The norm of the gradient at the solution vector is: ",
vnorm(grad),"\t", "The function Value at this point is: ",
fn(w, X, y), "\n")
return(w)
}
p1 <- proximal_gradient(negative_log_likelihood, log_likelihood_gradient, X, y,
max_iter = 1e10, tolerance = 0.1, steps = 100)
p1 <- proximal_gradient(negative_log_likelihood, log_likelihood_gradient, X, y,
max_iter = 1e10, tolerance = 0.1, steps = 100)
p1 <- proximal_gradient(negative_log_likelihood, log_likelihood_gradient, X, y,
max_iter = 1e10, tolerance = 0.01, steps = 100)
